"""
ENHANCED MALWARE TRAINING SCRIPT v2.0
Better hyperparameters • Threshold calibration • Reduced false positives
"""

import os
import numpy as np
import pandas as pd
import joblib
import json
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (classification_report, confusion_matrix, 
                             accuracy_score, precision_score, recall_score, f1_score,
                             roc_curve, auc)
import warnings
warnings.filterwarnings("ignore")

print("""
╔═══════════════════════════════════════════════════════════════════╗
║     ENHANCED MALWARE TRAINING v2.0                                ║
║     Optimized for Low False Positives                             ║
╚═══════════════════════════════════════════════════════════════════╝
""")

# ═══════════════════════════════════════════════════════════════════
# CONFIGURATION
# ═══════════════════════════════════════════════════════════════════

CSV_PATH = "enhanced_training_database.csv"
MODELS_DIR = "trained_models"
RANDOM_STATE = 42

# Create output directory
os.makedirs(MODELS_DIR, exist_ok=True)

# ═══════════════════════════════════════════════════════════════════
# LOAD DATA
# ═══════════════════════════════════════════════════════════════════

print("\n[1/6] Loading enhanced training data...")

if not os.path.exists(CSV_PATH):
    print(f"❌ ERROR: {CSV_PATH} not found!")
    print("   Run: python generate_enhanced_database.py first")
    exit(1)

df = pd.read_csv(CSV_PATH)
print(f"   ✓ Loaded {len(df):,} samples")

# Features
FEATURE_COLS = [c for c in df.columns if c not in ['sample_id', 'class', 'label']]
X = df[FEATURE_COLS].values
y = df['label'].values
class_names = df['class'].unique()

print(f"   ✓ Features: {len(FEATURE_COLS)}")
print(f"   ✓ Classes: {len(class_names)}")

print("\n   Class Distribution:")
for cls in sorted(class_names):
    count = (df['class'] == cls).sum()
    pct = count / len(df) * 100
    print(f"     {cls:12s}: {count:5,} ({pct:5.1f}%)")

# ═══════════════════════════════════════════════════════════════════
# SPLIT DATA
# ═══════════════════════════════════════════════════════════════════

print("\n[2/6] Splitting data (stratified)...")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.15, random_state=RANDOM_STATE, stratify=y_train
)

print(f"   ✓ Train: {len(X_train):,} samples")
print(f"   ✓ Val:   {len(X_val):,} samples")
print(f"   ✓ Test:  {len(X_test):,} samples")

# Scale features
print("\n[3/6] Scaling features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
print("   ✓ StandardScaler fitted")

# ═══════════════════════════════════════════════════════════════════
# TRAIN RANDOM FOREST
# ═══════════════════════════════════════════════════════════════════

print("\n[4/6] Training Random Forest (optimized)...")

# BETTER hyperparameters for low FP
rf = RandomForestClassifier(
    n_estimators=400,         # More trees = more stable
    max_depth=20,             # Limit depth to avoid overfitting
    min_samples_split=10,     # Higher = more conservative
    min_samples_leaf=5,       # Higher = more conservative
    max_features='sqrt',
    class_weight='balanced',  # Handle class imbalance
    bootstrap=True,
    oob_score=True,
    n_jobs=-1,
    random_state=RANDOM_STATE
)

rf.fit(X_train, y_train)

rf_val_pred = rf.predict(X_val)
rf_val_acc = accuracy_score(y_val, rf_val_pred)
rf_val_f1 = f1_score(y_val, rf_val_pred, average='weighted')

print(f"   ✓ OOB Score: {rf.oob_score_:.4f}")
print(f"   ✓ Val Accuracy: {rf_val_acc:.4f} ({rf_val_acc*100:.1f}%)")
print(f"   ✓ Val F1 (weighted): {rf_val_f1:.4f}")

# ═══════════════════════════════════════════════════════════════════
# TRAIN GRADIENT BOOSTING
# ═══════════════════════════════════════════════════════════════════

print("\n[5/6] Training Gradient Boosting (optimized)...")

gb = GradientBoostingClassifier(
    n_estimators=300,
    learning_rate=0.05,       # Lower = more conservative
    max_depth=6,              # Moderate depth
    min_samples_split=15,     # Conservative splits
    min_samples_leaf=6,       # Conservative leaves
    subsample=0.8,
    max_features='sqrt',
    random_state=RANDOM_STATE
)

gb.fit(X_train, y_train)

gb_val_pred = gb.predict(X_val)
gb_val_acc = accuracy_score(y_val, gb_val_pred)
gb_val_f1 = f1_score(y_val, gb_val_pred, average='weighted')

print(f"   ✓ Val Accuracy: {gb_val_acc:.4f} ({gb_val_acc*100:.1f}%)")
print(f"   ✓ Val F1 (weighted): {gb_val_f1:.4f}")

# ═══════════════════════════════════════════════════════════════════
# THRESHOLD CALIBRATION (CRITICAL FOR LOW FP)
# ═══════════════════════════════════════════════════════════════════

print("\n[6/6] Calibrating decision thresholds...")

# Get probabilities for benign class (label 0)
rf_val_proba = rf.predict_proba(X_val)
gb_val_proba = gb.predict_proba(X_val)

# Calculate ensemble probabilities
ensemble_proba = (rf_val_proba * 0.5 + gb_val_proba * 0.5)

# Find optimal threshold on VALIDATION set
# We want HIGH precision for malicious (label != 0)
binary_y_val = (y_val != 0).astype(int)  # 0=benign, 1=malicious
binary_proba = 1 - ensemble_proba[:, 0]  # Probability of malicious

# Test different thresholds
thresholds = np.arange(0.3, 0.8, 0.05)
best_threshold = 0.5
best_f1 = 0

print("\n   Testing thresholds on validation set:")
for thresh in thresholds:
    preds = (binary_proba >= thresh).astype(int)
    prec = precision_score(binary_y_val, preds, zero_division=0)
    rec = recall_score(binary_y_val, preds, zero_division=0)
    f1 = f1_score(binary_y_val, preds, zero_division=0)
    
    # Count false positives
    fp = ((preds == 1) & (binary_y_val == 0)).sum()
    fp_rate = fp / (binary_y_val == 0).sum() if (binary_y_val == 0).sum() > 0 else 0
    
    print(f"     Threshold {thresh:.2f}: Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}, FP_rate={fp_rate:.3f}")
    
    # Prefer threshold with low FP rate and good F1
    if f1 > best_f1 and fp_rate < 0.10:  # Less than 10% FP
        best_f1 = f1
        best_threshold = thresh

print(f"\n   ✓ Selected threshold: {best_threshold:.2f}")
print(f"   ✓ This minimizes false positives while maintaining detection")

# ═══════════════════════════════════════════════════════════════════
# EVALUATE ON TEST SET
# ═══════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("FINAL EVALUATION ON TEST SET")
print("="*70)

rf_test_pred = rf.predict(X_test)
gb_test_pred = gb.predict(X_test)

# Ensemble prediction with calibrated threshold
rf_test_proba = rf.predict_proba(X_test)
gb_test_proba = gb.predict_proba(X_test)
ensemble_test_proba = (rf_test_proba * 0.5 + gb_test_proba * 0.5)

# Binary classification with threshold
binary_y_test = (y_test != 0).astype(int)
binary_test_proba = 1 - ensemble_test_proba[:, 0]
ensemble_binary_pred = (binary_test_proba >= best_threshold).astype(int)

# Calculate metrics
test_acc = accuracy_score(binary_y_test, ensemble_binary_pred)
test_prec = precision_score(binary_y_test, ensemble_binary_pred, zero_division=0)
test_rec = recall_score(binary_y_test, ensemble_binary_pred, zero_division=0)
test_f1 = f1_score(binary_y_test, ensemble_binary_pred, zero_division=0)

# False positive analysis
fp_mask = (ensemble_binary_pred == 1) & (binary_y_test == 0)
fp_count = fp_mask.sum()
benign_count = (binary_y_test == 0).sum()
fp_rate = fp_count / benign_count if benign_count > 0 else 0

# False negative analysis
fn_mask = (ensemble_binary_pred == 0) & (binary_y_test == 1)
fn_count = fn_mask.sum()
malicious_count = (binary_y_test == 1).sum()
fn_rate = fn_count / malicious_count if malicious_count > 0 else 0

print(f"\nOverall Metrics:")
print(f"  Accuracy:  {test_acc:.4f} ({test_acc*100:.1f}%)")
print(f"  Precision: {test_prec:.4f} ({test_prec*100:.1f}%)")
print(f"  Recall:    {test_rec:.4f} ({test_rec*100:.1f}%)")
print(f"  F1-Score:  {test_f1:.4f}")

print(f"\nError Analysis:")
print(f"  False Positives: {fp_count}/{benign_count} ({fp_rate*100:.2f}%) ← Lower is better")
print(f"  False Negatives: {fn_count}/{malicious_count} ({fn_rate*100:.2f}%)")

print(f"\nConfusion Matrix (Binary):")
cm = confusion_matrix(binary_y_test, ensemble_binary_pred)
print(f"                Predicted")
print(f"               Benign  Malicious")
print(f"  True Benign    {cm[0,0]:4d}    {cm[0,1]:4d}  ← FP={cm[0,1]}")
print(f"  True Malicious {cm[1,0]:4d}    {cm[1,1]:4d}  ← FN={cm[1,0]}")

# ═══════════════════════════════════════════════════════════════════
# SAVE MODELS
# ═══════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("SAVING MODELS")
print("="*70)

joblib.dump(rf, f"{MODELS_DIR}/random_forest.pkl")
joblib.dump(gb, f"{MODELS_DIR}/gradient_boost.pkl")
joblib.dump(scaler, f"{MODELS_DIR}/scaler.pkl")

print(f"   ✓ random_forest.pkl")
print(f"   ✓ gradient_boost.pkl")
print(f"   ✓ scaler.pkl")

# Save configuration
config = {
    "feature_columns": FEATURE_COLS,
    "n_features": len(FEATURE_COLS),
    "n_classes": len(class_names),
    "classes": list(class_names),
    "threshold": float(best_threshold),
    "trained_at": datetime.now().isoformat(),
    "training_samples": len(X_train),
    "test_metrics": {
        "accuracy": float(test_acc),
        "precision": float(test_prec),
        "recall": float(test_rec),
        "f1_score": float(test_f1),
        "fp_rate": float(fp_rate),
        "fn_rate": float(fn_rate),
    },
    "hyperparameters": {
        "rf": {
            "n_estimators": 400,
            "max_depth": 20,
            "min_samples_split": 10,
            "min_samples_leaf": 5,
        },
        "gb": {
            "n_estimators": 300,
            "learning_rate": 0.05,
            "max_depth": 6,
            "min_samples_split": 15,
            "min_samples_leaf": 6,
        }
    }
}

with open(f"{MODELS_DIR}/model_config.json", "w") as f:
    json.dump(config, f, indent=2)

print(f"   ✓ model_config.json")

# ═══════════════════════════════════════════════════════════════════
# SUMMARY
# ═══════════════════════════════════════════════════════════════════

print("\n" + "="*70)
print("✅ TRAINING COMPLETE!")
print("="*70)

print(f"""
RESULTS SUMMARY:
  Test Accuracy:     {test_acc*100:.1f}%
  Precision:         {test_prec*100:.1f}% (of flagged files, % truly malicious)
  Recall:            {test_rec*100:.1f}% (of malware, % detected)
  F1-Score:          {test_f1:.3f}
  
  FALSE POSITIVE RATE: {fp_rate*100:.2f}% ← KEY METRIC (lower is better)
  False positives:     {fp_count} out of {benign_count} benign files
  
  Decision Threshold:  {best_threshold:.2f} (calibrated for low FP)

KEY IMPROVEMENTS vs v1.0:
  ✓ 5000 benign samples (was 2000)
  ✓ More conservative features (lower entropy for benign)
  ✓ Better hyperparameters (deeper trees, conservative splits)
  ✓ Calibrated threshold ({best_threshold:.2f} instead of 0.50)
  ✓ False positive rate: ~{fp_rate*100:.1f}% (was ~10-20%)

NEXT STEPS:
  1. Test on real files: python predict.py --file yourfile.exe
  2. If still too many FPs, increase threshold in model_config.json
  3. If missing malware, decrease threshold

Models saved to: {MODELS_DIR}/
""")

print("="*70)
